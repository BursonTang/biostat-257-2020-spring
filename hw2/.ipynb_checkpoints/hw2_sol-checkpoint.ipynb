{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Biostat 257 Homework 2\n",
    "\n",
    "**Due May 1 @ 11:59PM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Julia Version 1.4.1\n",
      "Commit 381693d3df* (2020-04-14 17:20 UTC)\n",
      "Platform Info:\n",
      "  OS: Windows (x86_64-w64-mingw32)\n",
      "  CPU: Intel(R) Core(TM) i7-8750H CPU @ 2.20GHz\n",
      "  WORD_SIZE: 64\n",
      "  LIBM: libopenlibm\n",
      "  LLVM: libLLVM-8.0.1 (ORCJIT, skylake)\n",
      "Environment:\n",
      "  JULIA = D:\\Julia-1.4.0\\bin\n"
     ]
    }
   ],
   "source": [
    "versioninfo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a linear mixed effects model\n",
    "$$\n",
    "    \\mathbf{Y}_i = \\mathbf{X}_i \\boldsymbol{\\beta} + \\mathbf{Z}_i \\boldsymbol{\\gamma} + \\boldsymbol{\\epsilon}_i, \\quad i=1,\\ldots,n,\n",
    "$$\n",
    "where   \n",
    "- $\\mathbf{Y}_i \\in \\mathbb{R}^{n_i}$ is the response vector of $i$-th individual,  \n",
    "- $\\mathbf{X}_i \\in \\mathbb{R}^{n_i \\times p}$ is the fixed effect predictor matrix of $i$-th individual,  \n",
    "- $\\mathbf{Z}_i \\in \\mathbb{R}^{n_i \\times q}$ is the random effect predictor matrix of $i$-th individual,  \n",
    "- $\\boldsymbol{\\epsilon}_i \\in \\mathbb{R}^{n_i}$ are multivariate normal $N(\\mathbf{0}_{n_i},\\sigma^2 \\mathbf{I}_{n_i})$,  \n",
    "- $\\boldsymbol{\\beta} \\in \\mathbb{R}^p$ are fixed effects, and  \n",
    "- $\\boldsymbol{\\gamma} \\in \\mathbb{R}^q$ are random effects assumed to be $N(\\mathbf{0}_q, \\boldsymbol{\\Sigma}_{q \\times q}$) independent of $\\boldsymbol{\\epsilon}_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1 Formula (10 pts)\n",
    "\n",
    "Write down the log-likelihood of the $i$-th datum $(\\mathbf{y}_i, \\mathbf{X}_i, \\mathbf{Z}_i)$ given parameters $(\\boldsymbol{\\beta}, \\boldsymbol{\\Sigma}, \\sigma^2)$.\n",
    "\n",
    "**Solution:** \n",
    "\n",
    "From givin information we know, $\\bar {Y} = X\\beta$ and $Var(Y) = \\sigma^2I_n + Z\\Sigma Z^T$, and thus we can calculate the log likelihood function as:\n",
    "\\begin{equation}\\begin{split}\n",
    "l & = -\\frac{n}{2}\\ln(2\\pi)-\\frac{1}{2}\\ln( |Var(Y_i)|) - \\frac{1}{2}(Y-\\mu)^TVar(Y_i)^{-1}(Y-\\mu)\\\\\n",
    "& = -\\frac{n}{2}\\ln(2\\pi)-\\frac{1}{2}\\ln (|\\sigma^2I_n + Z\\Sigma Z^T|) - \\frac{1}{2}(Y-X\\beta)^T(\\sigma^2I_n + Z\\Sigma Z^T)^{-1}(Y-X\\beta)\n",
    "\\end{split}\\end{equation}\n",
    "In this case we have the **Variance** of y as: \n",
    "$$\\sigma^2I_n + Z\\Sigma Z^T = \\sigma^2I_n + ZLL^TZ^T$$\n",
    "which can be recognized as an \"easy + low rank\" structure.\n",
    "\n",
    "\n",
    "Also we can identify $A = \\sigma^2I_n$ and $U = V = ZL$ and thus utilize the **Woodbury formula** and determinantal fomula in HW1 Q5. By Woodbury, the inverse of the covariance matrix is:\n",
    "$$(\\sigma^2I_n + ZLL^TZ^T)^{-1} = \\sigma^{-2}I_n - \\sigma^{-4}ZL(I_q + \\sigma^{-2}L^TZ^TZL)^{-1}L^TZ^T$$\n",
    "And we can calculate the $(ZL)^TZL$ in the order of $Z^TZ$ (q\\*n X n\\*q => q\\*q), $L^TZ^TZ$ (q\\*q X q\\*q), $L^TZ^TZL$ (q\\*q X q\\*q) to get the lowest computational cost. Proper implementation of this function will cost $\\mathcal{O}(np)$ (calculating the residual vector $r = y-X\\beta$) plus $\\mathcal{O}(nq)$ (calculating $Z^Tr$) flops.\n",
    "\n",
    "And we can further cut the cost by pre-computing certain quantities in light of identities\n",
    "$$r^Tr = y^Ty + \\beta^TX^TX\\beta - 2y^TX\\beta$$\n",
    "and\n",
    "$$Z^Tr = Z^Ty - Z^TX\\beta$$\n",
    "So each evaluation costs $\\mathcal{O}(p^2) + \\mathcal{O}(q^3) +\\mathcal{O}(pq)$ flops, and in real application, these variables are all small numbers.\n",
    "\n",
    "**Note:** we can only pre-allocate/pre-calculate varibales related to data, like X, y, but those variable related with model parameters should not be pre-calculated, since we need to adjust these parameters to validate the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2 Start-up code\n",
    "\n",
    "Use the following template to define a type `LmmObs` that holds an LMM datum $(\\mathbf{y}_i, \\mathbf{X}_i, \\mathbf{Z}_i)$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LmmObs"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define a type that holds LMM datum\n",
    "struct LmmObs{T <: AbstractFloat}\n",
    "    # data\n",
    "    y :: Vector{T}\n",
    "    X :: Matrix{T}\n",
    "    Z :: Matrix{T}\n",
    "    # working arrays\n",
    "    # whatever intermediate arrays you may want to pre-allocate\n",
    "#     yty         :: T\n",
    "#     xty         :: Vector{T}\n",
    "#     zty         :: Vector{T}\n",
    "#     xtx         :: Matrix{T}\n",
    "#     ztx         :: Matrix{T}\n",
    "    res         :: Vector{T}\n",
    "    storage_q   :: Vector{T}\n",
    "    storage_q2  :: Vector{T}\n",
    "    ztz         :: Matrix{T}\n",
    "    storage_qq  :: Matrix{T}\n",
    "end\n",
    "\n",
    "# constructor\n",
    "function LmmObs(\n",
    "        y::Vector{T}, \n",
    "        X::Matrix{T}, \n",
    "        Z::Matrix{T}) where T <: AbstractFloat\n",
    "#     yty        = abs2(norm(y))\n",
    "#     xty        = transpose(x) * y\n",
    "#     zty        = transpose(z) * y\n",
    "#     xtx        = transpose(x) * x\n",
    "#     ztx        = transpose(x) * x\n",
    "    res        = similar(y)\n",
    "    storage_q  = Vector{T}(undef, size(Z, 2))\n",
    "    storage_q2 = Vector{T}(undef, size(Z, 2))\n",
    "    ztz        = transpose(Z) * Z\n",
    "    storage_qq = similar(ztz)\n",
    "    LmmObs(y, X, Z, res, storage_q, storage_q2, ztz, storage_qq)\n",
    "#     LmmObs(y, X, Z, yty, xty, zty, xtx, ztx,\n",
    "#         storage_q, storage_q2, ztz, storage_qq)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function, with interface   \n",
    "```julia\n",
    "logl!(obs, β, L, σ²)\n",
    "```\n",
    "that evaluates the log-likelihood of the $i$-th datum. Here `L` is the lower triangular Cholesky factor from the Cholesky decomposition `Σ=LL'`. Make your code efficient in the $n_i \\gg q$ case. Think the intensive longitudinal measurement setting.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "logl! (generic function with 1 method)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function logl!(\n",
    "        obs :: LmmObs{T}, \n",
    "        β   :: Vector{T}, \n",
    "        L   :: Matrix{T}, \n",
    "        σ²  :: T) where T <: AbstractFloat\n",
    "    n, p, q = size(obs.X, 1), size(obs.X, 2), size(obs.Z, 2)    \n",
    "\n",
    "    # TODO: compute and return the log-likelihood\n",
    "    \n",
    "    # Calculate residual\n",
    "    BLAS.gemv!('N', T(1), obs.X, β, T(-1), copy!(obs.res, obs.y))\n",
    "    # IF the structure obs.res = copy(y), but this will make res a shadow of y, and y will change with res\n",
    "#     BLAS.gemv!('N', T(1), obs.X, β, T(-1), obs.res)\n",
    "    \n",
    "#     mul!(obs.res, obs.X, β)  # bottleneck\n",
    "#     obs.res .= obs.y .- obs.res\n",
    "    \n",
    "    #Z'A => L'Z'A  = (ZL)'A\n",
    "    mul!(obs.storage_q2, transpose(obs.Z), obs.res)\n",
    "    mul!(obs.storage_q, transpose(L), obs.storage_q2)\n",
    "    \n",
    "    # calculate (ZL)'ZL\n",
    "    mul!(obs.storage_qq, transpose(L), obs.ztz)\n",
    "    rmul!(obs.storage_qq,LowerTriangular(L))\n",
    "    \n",
    "    rdiv!(obs.storage_qq, σ²)\n",
    "    # obs.storage_qq ./= σ² \n",
    "    for j = 1:q\n",
    "        obs.storage_qq[j,j] += 1\n",
    "    end\n",
    "    # obs.storage_qq .+= Matrix{Float64}(I, q, q)\n",
    "    Ωchol = cholesky!(Symmetric(obs.storage_qq))\n",
    "    \n",
    "#     # determinate of the triangular matrix, also det(LU) = det(L)^2\n",
    "#     det = 1\n",
    "#     for j = 1:q\n",
    "#         det *= obs.storage_qq[j,j] \n",
    "#     end\n",
    "    \n",
    "    l= (-(n//2) * log(2π)\n",
    "#         - (1//2)*( log(det^2)+n*log(σ²) )\n",
    "        - (1//2)*( logdet(Ωchol) + n*log(σ²) )\n",
    "        - (1//2)*( dot(obs.res, obs.res)/σ² -\n",
    "            (1/(σ²*σ²))*dot(obs.storage_q, Ωchol\\obs.storage_q) )\n",
    "    )\n",
    "\n",
    "    return l\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hint**: This function shouldn't be very long. Mine, obeying 80-character rule, is 25 lines. If you find yourself writing very long code, you're on the wrong track. Think about algorithm first then use BLAS functions to reduce memory allocations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3 Correctness (15 pts)\n",
    "\n",
    "Compare your result (both accuracy and timing) to the [Distributions.jl](https://juliastats.org/Distributions.jl/stable/multivariate/#Distributions.AbstractMvNormal) package using following data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "using BenchmarkTools, Distributions, LinearAlgebra, Random\n",
    "\n",
    "Random.seed!(257)\n",
    "# dimension\n",
    "n, p, q = 2000, 5, 3;\n",
    "\n",
    "# predictors\n",
    "X  = [ones(n) randn(n, p - 1)];\n",
    "Z  = [ones(n) randn(n, q - 1)];\n",
    "# parameter values\n",
    "β  = [2.0; -1.0; rand(p - 2)];\n",
    "σ² = 1.5;\n",
    "Σ  = fill(0.1, q, q) + 0.9I;\n",
    "# generate y\n",
    "y  = X * β + Z * rand(MvNormal(Σ)) + sqrt(σ²) * randn(n);\n",
    "\n",
    "# form an LmmObs object\n",
    "obs = LmmObs(y, X, Z);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the standard way to evaluate log-density of a multivariate normal, using the Distributions.jl package. Let's evaluate the log-likelihood of this datum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-3247.456858063827"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "μ  = X * β\n",
    "Ω  = Z * Σ * transpose(Z) +  σ² * I\n",
    "\n",
    "# From istribution pacakage:\n",
    "mvn = MvNormal(μ, Symmetric(Ω)) # MVN(μ, Σ)\n",
    "logpdf(mvn, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that your answer matches that from Distributions.jl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-3247.4568580638293"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L = Matrix(cholesky(Σ).L)\n",
    "logl!(obs, β, L, σ²)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You will lose all 15 + 30 + 30 = 75 points** if the following statement throws `AssertError`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@assert logl!(obs, β, Matrix(cholesky(Σ).L), σ²) ≈ logpdf(mvn, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4 Efficiency (30 pts)\n",
    "\n",
    "Benchmarking your code and compare to the Distributions.jl function `logpdf`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: \n",
       "  memory estimate:  30.55 MiB\n",
       "  allocs estimate:  5\n",
       "  --------------\n",
       "  minimum time:     17.297 ms (0.00% GC)\n",
       "  median time:      19.323 ms (0.00% GC)\n",
       "  mean time:        21.360 ms (11.06% GC)\n",
       "  maximum time:     29.443 ms (22.08% GC)\n",
       "  --------------\n",
       "  samples:          234\n",
       "  evals/sample:     1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# benchmark the `logpdf` function in Distribution.jl\n",
    "bm1 = @benchmark logpdf($mvn, $y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Count  Overhead File                    Line Function\n",
      " =====  ======== ====                    ==== ========\n",
      "   212         0 In[10]                     4 macro expansion\n",
      "   213         0 In[10]                     4 top-level scope\n",
      "   178         0 In[2]                     14 logl!(::LmmObs{Float64}, ::Arra...\n",
      "    10         0 In[2]                     19 logl!(::LmmObs{Float64}, ::Arra...\n",
      "     3         0 In[2]                     20 logl!(::LmmObs{Float64}, ::Arra...\n",
      "     3         0 In[2]                     28 logl!(::LmmObs{Float64}, ::Arra...\n",
      "     2         0 In[2]                     36 logl!(::LmmObs{Float64}, ::Arra...\n",
      "    16         0 In[2]                     44 logl!(::LmmObs{Float64}, ::Arra...\n",
      "     1         0 @Base\\array.jl           313 copyto!(::Array{Float64,1}, ::I...\n",
      "     1         0 @Base\\array.jl           318 copyto!(::Array{Float64,1}, ::I...\n",
      "     3         1 @Base\\array.jl           330 copyto!\n",
      "     3         0 @Base\\array.jl           361 similar\n",
      "     1         1 @Base\\array.jl           271 unsafe_copyto!\n",
      "     3         3 @Base\\boot.jl            405 Array\n",
      "     3         0 @Base\\boot.jl            414 Array\n",
      "   213         0 @Base\\boot.jl            331 eval\n",
      "   213         0 @Base\\essentials.jl      712 #invokelatest#1\n",
      "   213         0 @Base\\essentials.jl      711 invokelatest\n",
      "     2         2 @Base\\promotion.jl       398 ==\n",
      "     1         0 @Base\\range.jl           597 iterate\n",
      "   213         0 @Base\\task.jl            358 (::IJulia.var\"#15#18\")()\n",
      "   213         0 ...ia\\src\\eventloop.jl     8 eventloop(::ZMQ.Socket)\n",
      "   213         0 ...\\execute_request.jl    67 execute_request(::ZMQ.Socket, :...\n",
      "   213         0 ...\\SoftGlobalScope.jl   218 softscope_include_string(::Modu...\n",
      "     4         4 ...Algebra\\src\\blas.jl   286 dot\n",
      "     1         1 ...Algebra\\src\\blas.jl   330 dot(::Array{Float64,1}, ::Array...\n",
      "     4         0 ...Algebra\\src\\blas.jl   335 dot(::Array{Float64,1}, ::Array...\n",
      "   188       188 ...Algebra\\src\\blas.jl   587 gemv!(::Char, ::Float64, ::Arra...\n",
      "     3         3 ...Algebra\\src\\blas.jl  1670 trmm!(::Char, ::Char, ::Char, :...\n",
      "     2         1 ...bra\\src\\cholesky.jl   225 cholesky!(::Symmetric{Float64,A...\n",
      "     1         0 ...bra\\src\\cholesky.jl   144 _chol!\n",
      "     2         0 ...bra\\src\\cholesky.jl   225 cholesky!\n",
      "     5         0 ...bra\\src\\cholesky.jl   471 ldiv!\n",
      "     3         0 ...rc\\factorization.jl    99 \\\n",
      "     3         0 ...rc\\factorization.jl   100 \\\n",
      "     5         0 ...rc\\factorization.jl   101 \\\n",
      "     1         1 ...gebra\\src\\lapack.jl  3077 potrf!\n",
      "     5         5 ...gebra\\src\\lapack.jl  3127 potrs!(::Char, ::Array{Float64,...\n",
      "     5         0 ...gebra\\src\\matmul.jl     9 dot\n",
      "     3         3 ...gebra\\src\\matmul.jl   454 gemv!(::Array{Float64,1}, ::Cha...\n",
      "    10         0 ...gebra\\src\\matmul.jl   470 gemv!(::Array{Float64,1}, ::Cha...\n",
      "    13         0 ...gebra\\src\\matmul.jl    97 mul!\n",
      "    13         0 ...gebra\\src\\matmul.jl   208 mul!\n",
      "     3         0 ...a\\src\\triangular.jl   732 rmul!\n",
      "   213         0 ...file\\src\\Profile.jl    28 macro expansion\n",
      "Total snapshots: 217\n"
     ]
    }
   ],
   "source": [
    "using Profile\n",
    "Profile.clear()\n",
    "Profile.init()\n",
    "@profile for i in 1:10000; logl!(obs, β, L, σ²); end\n",
    "Profile.print(format=:flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: \n",
       "  memory estimate:  224 bytes\n",
       "  allocs estimate:  5\n",
       "  --------------\n",
       "  minimum time:     54.400 μs (0.00% GC)\n",
       "  median time:      63.400 μs (0.00% GC)\n",
       "  mean time:        66.537 μs (0.00% GC)\n",
       "  maximum time:     530.000 μs (0.00% GC)\n",
       "  --------------\n",
       "  samples:          10000\n",
       "  evals/sample:     1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# benchmark your implementation\n",
    "L = Matrix(cholesky(Σ).L)\n",
    "bm2 = @benchmark logl!($obs, $β, $L, $σ²)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The points you will get is\n",
    "$$\n",
    "\\frac{x}{1000} \\times 30,\n",
    "$$\n",
    "where $x$ is the speedup of your program against the standard method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.143257097791798"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is the points you'll get\n",
    "clamp(median(bm1).time / median(bm2).time / 1000 * 30, 0, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "304.77523659305996"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "median(bm1).time / median(bm2).time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hint**: Apparently I am using 1000 as denominator because I expect your code to be at least $1000 \\times$ faster than the standard method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q5 Memory (30 pts)\n",
    "\n",
    "You want to avoid memory allocation in the \"hot\" function `logl!`. You will lose 1 point for each `1 KiB = 1024 bytes` memory allocation. In other words, the points you get for this question is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29.78125"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clamp(30 - median(bm2).memory / 1024, 0, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hint**: I am able to reduce the memory allocation to 0 bytes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q6 Misc (15 pts)\n",
    "\n",
    "Coding style, Git workflow, etc. For reproducibity, make sure we (TA and myself) can run your Jupyter Notebook. That is how we grade Q4 and Q5. If we cannot run it, you will get zero points."
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.4.1",
   "language": "julia",
   "name": "julia-1.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.4.1"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "87px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
