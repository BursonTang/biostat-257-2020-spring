{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Biostat 257 Homework 2\n",
    "\n",
    "**Due May 1 @ 11:59PM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "versioninfo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a linear mixed effects model\n",
    "$$\n",
    "    \\mathbf{Y}_i = \\mathbf{X}_i \\boldsymbol{\\beta} + \\mathbf{Z}_i \\boldsymbol{\\gamma} + \\boldsymbol{\\epsilon}_i, \\quad i=1,\\ldots,n,\n",
    "$$\n",
    "where   \n",
    "- $\\mathbf{Y}_i \\in \\mathbb{R}^{n_i}$ is the response vector of $i$-th individual,  \n",
    "- $\\mathbf{X}_i \\in \\mathbb{R}^{n_i \\times p}$ is the fixed effect predictor matrix of $i$-th individual,  \n",
    "- $\\mathbf{Z}_i \\in \\mathbb{R}^{n_i \\times q}$ is the random effect predictor matrix of $i$-th individual,  \n",
    "- $\\boldsymbol{\\epsilon}_i \\in \\mathbb{R}^{n_i}$ are multivariate normal $N(\\mathbf{0}_{n_i},\\sigma^2 \\mathbf{I}_{n_i})$,  \n",
    "- $\\boldsymbol{\\beta} \\in \\mathbb{R}^p$ are fixed effects, and  \n",
    "- $\\boldsymbol{\\gamma} \\in \\mathbb{R}^q$ are random effects assumed to be $N(\\mathbf{0}_q, \\boldsymbol{\\Sigma}_{q \\times q}$) independent of $\\boldsymbol{\\epsilon}_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1 Formula (10 pts)\n",
    "\n",
    "Write down the log-likelihood of the $i$-th datum $(\\mathbf{y}_i, \\mathbf{X}_i, \\mathbf{Z}_i)$ given parameters $(\\boldsymbol{\\beta}, \\boldsymbol{\\Sigma}, \\sigma^2)$. \\\n",
    "**Solution:** The mean and variance of the response vector is:\n",
    "\n",
    "From givin information we know, $\\bar {Y} = X\\beta$ and $Var(Y) = \\sigma^2I_n + Z\\Sigma Z^T$, and thus we can calculate the log likelihood function as:\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "l & = -\\frac{n}{2}\\ln(2\\pi)-\\frac{1}{2}\\ln( |Var(Y_i)|) - \\frac{1}{2}(Y-\\mu)^TVar(Y_i)^{-1}(Y-\\mu)\\\\\n",
    "& = -\\frac{n}{2}\\ln(2\\pi)-\\frac{1}{2}\\ln (|\\sigma^2I_n + Z\\Sigma Z^T|) - \\frac{1}{2}(Y-X\\beta)^T(\\sigma^2I_n + Z\\Sigma Z^T)^{-1}(Y-X\\beta)\n",
    "\\end{split}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2 Start-up code\n",
    "\n",
    "Use the following template to define a type `LmmObs` that holds an LMM datum $(\\mathbf{y}_i, \\mathbf{X}_i, \\mathbf{Z}_i)$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LmmObs"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define a type that holds LMM datum\n",
    "struct LmmObs{T <: AbstractFloat}\n",
    "    # data\n",
    "    y :: Vector{T}\n",
    "    X :: Matrix{T}\n",
    "    Z :: Matrix{T}\n",
    "    # working arrays\n",
    "    # whatever intermediate arrays you may want to pre-allocate\n",
    "    res        :: Vector{T}\n",
    "    storage_q  :: Vector{T}\n",
    "    ztz        :: Matrix{T}\n",
    "    storage_qq :: Matrix{T}\n",
    "end\n",
    "\n",
    "# constructor\n",
    "function LmmObs(\n",
    "        y::Vector{T}, \n",
    "        X::Matrix{T}, \n",
    "        Z::Matrix{T}) where T <: AbstractFloat\n",
    "    res        = similar(y)\n",
    "    storage_q  = Vector{T}(undef, size(Z, 2))\n",
    "    ztz        = transpose(Z) * Z\n",
    "    storage_qq = similar(ztz)\n",
    "    LmmObs(y, X, Z, res, storage_q, ztz, storage_qq)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function, with interface   \n",
    "```julia\n",
    "logl!(obs, β, L, σ²)\n",
    "```\n",
    "that evaluates the log-likelihood of the $i$-th datum. Here `L` is the lower triangular Cholesky factor from the Cholesky decomposition `Σ=LL'`. Make your code efficient in the $n_i \\gg q$ case. Think the intensive longitudinal measurement setting.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "logl! (generic function with 1 method)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function logl!(\n",
    "        obs :: LmmObs{T}, \n",
    "        β   :: Vector{T}, \n",
    "        L   :: Matrix{T}, \n",
    "        σ²  :: T) where T <: AbstractFloat\n",
    "    n, p, q = size(obs.X, 1), size(obs.X, 2), size(obs.Z, 2)    \n",
    "\n",
    "    # TODO: compute and return the log-likelihood\n",
    "    rmul!(obs.Z,LowerTriangular(L))\n",
    "#     Σ = σ²*I + obs.Z*transpose(obs.Z);\n",
    "    Σ = LinearAlgebra.BLAS.syrk!('U','N', 1.0, obs.Z, 1.0, σ²*Matrix{Float64}(I, n, n))\n",
    "    Σchol = cholesky!(Symmetric(Σ));\n",
    "    μin = obs.y-obs.X*β\n",
    "    l = -(n//2)*log(2*pi) - (1//2)*logdet(Σchol) -  (1//2) * dot(μin, Σchol\\μin)#transpose(μ)*inv(Var)*μ \n",
    "    return l\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hint**: This function shouldn't be very long. Mine, obeying 80-character rule, is 25 lines. If you find yourself writing very long code, you're on the wrong track. Think about algorithm first then use BLAS functions to reduce memory allocations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3 Correctness (15 pts)\n",
    "\n",
    "Compare your result (both accuracy and timing) to the [Distributions.jl](https://juliastats.org/Distributions.jl/stable/multivariate/#Distributions.AbstractMvNormal) package using following data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "using BenchmarkTools, Distributions, LinearAlgebra, Random\n",
    "\n",
    "Random.seed!(257)\n",
    "# dimension\n",
    "n, p, q = 2000, 5, 3;\n",
    "\n",
    "# predictors\n",
    "X  = [ones(n) randn(n, p - 1)];\n",
    "Z  = [ones(n) randn(n, q - 1)];\n",
    "# parameter values\n",
    "β  = [2.0; -1.0; rand(p - 2)];\n",
    "σ² = 1.5;\n",
    "Σ  = fill(0.1, q, q) + 0.9I;\n",
    "# generate y\n",
    "y  = X * β + Z * rand(MvNormal(Σ)) + sqrt(σ²) * randn(n);\n",
    "\n",
    "# form an LmmObs object\n",
    "obs = LmmObs(y, X, Z);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the standard way to evaluate log-density of a multivariate normal, using the Distributions.jl package. Let's evaluate the log-likelihood of this datum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-3247.262248197789"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "μ  = X * β\n",
    "Ω  = Z * Σ * transpose(Z) +  σ² * I\n",
    "\n",
    "# From istribution pacakage:\n",
    "mvn = MvNormal(μ, Symmetric(Ω)) # MVN(μ, Σ)\n",
    "logpdf(mvn, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000×2000 Array{Float64,2}:\n",
       " 3.90635  0.195727  -0.1906    2.66111   …  0.810258   0.863338    2.02967\n",
       " 0.0      3.00061    1.29769   0.538609     1.09562    1.36188     0.59826\n",
       " 0.0      0.0        4.73628  -2.28328      1.0485    -0.146293   -0.638801\n",
       " 0.0      0.0        0.0       7.21102      0.88839    2.63182     3.32422\n",
       " 0.0      0.0        0.0       0.0          1.02721    3.49556     3.26985\n",
       " 0.0      0.0        0.0       0.0       …  0.894418   0.909553    1.50334\n",
       " 0.0      0.0        0.0       0.0          0.773802   1.36463     2.79639\n",
       " 0.0      0.0        0.0       0.0          0.644937  -0.0519489   2.21156\n",
       " 0.0      0.0        0.0       0.0          1.03901    0.565792    0.161774\n",
       " 0.0      0.0        0.0       0.0          0.901547   2.09993     2.68483\n",
       " 0.0      0.0        0.0       0.0       …  0.893249   1.76382     2.3941\n",
       " 0.0      0.0        0.0       0.0          0.861202  -0.886445   -0.126038\n",
       " 0.0      0.0        0.0       0.0          0.866513   0.521893    1.29309\n",
       " ⋮                                       ⋱                        \n",
       " 0.0      0.0        0.0       0.0          1.18501    0.365321   -1.04134\n",
       " 0.0      0.0        0.0       0.0          1.03836    0.511667    0.11025\n",
       " 0.0      0.0        0.0       0.0       …  1.12603    0.747254   -0.244338\n",
       " 0.0      0.0        0.0       0.0          0.899433  -0.674644   -0.167958\n",
       " 0.0      0.0        0.0       0.0          1.08355    2.59927     1.95929\n",
       " 0.0      0.0        0.0       0.0          0.932511   1.37908     1.72869\n",
       " 0.0      0.0        0.0       0.0          0.8865     0.0379962   0.656698\n",
       " 0.0      0.0        0.0       0.0       …  0.903301  -0.141121    0.356994\n",
       " 0.0      0.0        0.0       0.0          1.01991    0.74473     0.476991\n",
       " 0.0      0.0        0.0       0.0          2.50679    1.06691     0.899437\n",
       " 0.0      0.0        0.0       0.0          0.0        3.65654     1.6153\n",
       " 0.0      0.0        0.0       0.0          0.0        0.0         3.69841"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L = Matrix(cholesky(Σ).L)\n",
    "Z  = [ones(n) randn(n, q - 1)];\n",
    "LinearAlgebra.BLAS.syrk!('U','N', 1.0, Z*L, 1.0, σ²*Matrix{Float64}(I, n, n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that your answer matches that from Distributions.jl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-3247.4568580638256"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L = Matrix(cholesky(Σ).L)\n",
    "logl!(obs, β, L, σ²)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You will lose all 15 + 30 + 30 = 75 points** if the following statement throws `AssertError`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "@assert logl!(obs, β, Matrix(cholesky(Σ).L), σ²) ≈ logpdf(mvn, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Count  Overhead File                    Line Function\n",
      " =====  ======== ====                    ==== ========\n",
      "     6         0 In[143]                    9 logl!(::LmmObs{Float64}, ::Arra...\n",
      "    10         0 In[143]                   10 logl!(::LmmObs{Float64}, ::Arra...\n",
      "     2         0 In[143]                   12 logl!(::LmmObs{Float64}, ::Arra...\n",
      "     5         5 @Base\\array.jl           352 copy\n",
      "    19         1 @Base\\boot.jl            331 eval\n",
      "    19         0 @Base\\essentials.jl      712 #invokelatest#1\n",
      "    19         0 @Base\\essentials.jl      711 invokelatest\n",
      "     1         0 @Base\\operators.jl       529 *\n",
      "    19         0 @Base\\task.jl            358 (::IJulia.var\"#15#18\")()\n",
      "    19         0 ...ia\\src\\eventloop.jl     8 eventloop(::ZMQ.Socket)\n",
      "    19         0 ...\\execute_request.jl    67 execute_request(::ZMQ.Socket, :...\n",
      "    19         0 ...\\SoftGlobalScope.jl   218 softscope_include_string(::Modu...\n",
      "     5         0 ...rc\\LinearAlgebra.jl   347 copy_oftype\n",
      "     1         1 ...Algebra\\src\\blas.jl  1167 gemm!(::Char, ::Char, ::Float64...\n",
      "    10         0 ...bra\\src\\cholesky.jl   225 cholesky!(::Symmetric{Float64,A...\n",
      "    10         0 ...bra\\src\\cholesky.jl   144 _chol!\n",
      "    10         0 ...bra\\src\\cholesky.jl   225 cholesky!\n",
      "    10         0 ...bra\\src\\cholesky.jl   225 cholesky!(::Symmetric{Float64,A...\n",
      "     2         0 ...bra\\src\\cholesky.jl   471 ldiv!\n",
      "     2         0 ...rc\\factorization.jl   101 \\(::Cholesky{Float64,Array{Floa...\n",
      "    10        10 ...gebra\\src\\lapack.jl  3077 potrf!\n",
      "     2         2 ...gebra\\src\\lapack.jl  3127 potrs!(::Char, ::Array{Float64,...\n",
      "     1         0 ...gebra\\src\\matmul.jl   153 *(::Array{Float64,2}, ::Transpo...\n",
      "     1         0 ...gebra\\src\\matmul.jl   597 gemm_wrapper!(::Array{Float64,2...\n",
      "     1         0 ...gebra\\src\\matmul.jl   208 mul!\n",
      "     1         0 ...gebra\\src\\matmul.jl   325 mul!\n",
      "     5         0 ...c\\uniformscaling.jl   117 +(::UniformScaling{Float64}, ::...\n",
      "     5         0 ...c\\uniformscaling.jl   165 +(::Array{Float64,2}, ::Uniform...\n",
      "Total snapshots: 24\n"
     ]
    }
   ],
   "source": [
    "using Profile\n",
    "\n",
    "Profile.clear()\n",
    "@profile logl!(obs, β, L, σ²)\n",
    "Profile.print(format=:flat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4 Efficiency (30 pts)\n",
    "\n",
    "Benchmarking your code and compare to the Distributions.jl function `logpdf`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: \n",
       "  memory estimate:  30.55 MiB\n",
       "  allocs estimate:  5\n",
       "  --------------\n",
       "  minimum time:     15.825 ms (0.00% GC)\n",
       "  median time:      16.269 ms (0.00% GC)\n",
       "  mean time:        18.826 ms (13.78% GC)\n",
       "  maximum time:     27.073 ms (35.44% GC)\n",
       "  --------------\n",
       "  samples:          266\n",
       "  evals/sample:     1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# benchmark the `logpdf` function in Distribution.jl\n",
    "bm1 = @benchmark logpdf($mvn, $y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: \n",
       "  memory estimate:  61.08 MiB\n",
       "  allocs estimate:  11\n",
       "  --------------\n",
       "  minimum time:     64.936 ms (0.00% GC)\n",
       "  median time:      71.102 ms (7.36% GC)\n",
       "  mean time:        71.121 ms (6.99% GC)\n",
       "  maximum time:     80.559 ms (12.83% GC)\n",
       "  --------------\n",
       "  samples:          71\n",
       "  evals/sample:     1"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# benchmark your implementation\n",
    "L = Matrix(cholesky(Σ).L)\n",
    "bm2 = @benchmark logl!($obs, $β, $L, $σ²)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The points you will get is\n",
    "$$\n",
    "\\frac{x}{1000} \\times 30,\n",
    "$$\n",
    "where $x$ is the speedup of your program against the standard method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.007720162732469542"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is the points you'll get\n",
    "clamp(median(bm1).time / median(bm2).time / 1000 * 30, 0, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hint**: Apparently I am using 1000 as denominator because I expect your code to be at least $1000 \\times$ faster than the standard method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q5 Memory (30 pts)\n",
    "\n",
    "You want to avoid memory allocation in the \"hot\" function `logl!`. You will lose 1 point for each `1 KiB = 1024 bytes` memory allocation. In other words, the points you get for this question is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clamp(30 - median(bm2).memory / 1024, 0, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hint**: I am able to reduce the memory allocation to 0 bytes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q6 Misc (15 pts)\n",
    "\n",
    "Coding style, Git workflow, etc. For reproducibity, make sure we (TA and myself) can run your Jupyter Notebook. That is how we grade Q4 and Q5. If we cannot run it, you will get zero points."
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.4.0",
   "language": "julia",
   "name": "julia-1.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.4.0"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "87px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
